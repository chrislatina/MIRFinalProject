% Template for ICASSP-2015 paper; to be used with:
%          spconf.sty  - ICASSP/ICIP LaTeX style file, and
%          IEEEbib.bst - IEEE bibliography style file.
% --------------------------------------------------------------------------
\documentclass{article}
\usepackage{spconf,amsmath,graphicx}
\usepackage{hyperref,tabularx}

%\setcounter{secnumdepth}{4}

%\titleformat{\paragraph}
%{\normalfont\normalsize\bfseries}{\theparagraph}{1em}{}
%\titlespacing*{\paragraph}
%{0pt}{3.25ex plus 1ex minus .2ex}{1.5ex plus .2ex}

% Example definitions.
% --------------------
\def\x{{\mathbf x}}
\def\L{{\cal L}}

% Title.
% ------
\title{Computing a Song's Decade}

%
% Single address.
% ---------------
%\name{Author(s) Name(s)\thanks{Thanks to XYZ agency for funding.}}
%\address{Author Affiliation(s)}
%
% For example:
% ------------
%\address{School\\
%	Department\\
%	Address}
%
% Two addresses (uncomment and modify for two-address case).
% ----------------------------------------------------------
\twoauthors
  {Christopher Latina}
	{Georgia Tech Center for Music Technology\\
	840 McMillan St. Atlanta, GA\\
	chris.latina@gatech.edu}
  {Liang Tang}
	{Georgia Tech Center for Music Technology\\
	840 McMillan St. Atlanta, GA\\
	liangmt@gatech.edu}
%
\begin{document}
%\ninept
%
\maketitle
%
\begin{abstract}



%Are there new insights through computational methods to the problem of getting song's year? This project is motivated by combining traditional music information retrieval (hereafter referred to as MIR) approach and metadata of song to compute a song's decade.  Our task is to determine whether a heirarchical approach to classification using genre can help predict other metadata about a set of audio clips, such as the year of release. While genre classification is a common MIR problem, we set out to investigate whether one can use audio features in conjunction with metadata cross referenced with Discogs to estimate a song's decade.

Is it possible to compute a song's decade from a short audio clip? This paper describes an algorithm that makes use of instantaneous audio features, metadata, and traditional machine learning algorithms to estimate a song's decade. Inspired by the accuracy and consistency of the Discogs database, we wanted to apply the approaches commonly used to classify genre to additional song and album metadata. Our task is to determine which features are most relevant for the task and more generically, whether a hierarchical approach to classification using genre can help predict other metadata about a set of audio clips, such as the year of release.

\end{abstract}
%
\begin{keywords}
Year Computation, Machine Learning, Audio Content Analysis, MIR, GTZAN
\end{keywords}
%
\section{Introduction}
\label{sec:intro}

MIR is an active field of machine learning research. Users of online music services are very likely to search for music by genre or style, so researchers have attempted to understand how to automatically classify music by these labels. For instance, overarching genres like rock or disco likely exhibit enough distinction for computers to effectively distinguish between them \cite{diab1musical}.

The dataset used for training and testing the machine learning model plays a significant role in its results. Many datasets that exist for genre classification only contain the minimum amount of information, namely the audio snippet and a label. Recently, researchers are taking effort to compile more complete and accurate datasets in different ways \cite{bertin2011million}. One approach is to supplement the existing data by providing music-related metadata information such as the song title, album name, year and even price of the release.

Discogs.com is an user-sourced database and marketplace for album releases on vinyl, cassette, CD, and digital formats. Unlike last.fm, the data is extremely complete and accurate because the website is heavily moderated. Notably, the Discogs API mentions that each release has a ``master format", typically the first release chronologically along with metadata such as genre, styles (subgenres), and year.

% maybe talk about digital7 here???

In this paper, the problem of automatically computing song's decade is addressed. More specifically, the technique of using the combination of traditional music genre classification approaches with music metadata to estimate the year of a song is proposed. Although there has been significant works in the development of music information retrieval especially for genre classification, there has been relatively little work in the development of song's year or decade computation.

%==========================================
\section{Related Work}
\label{sec:related}


%Style is a vague term that may be understood in different ways depending on the context in which it is employed. Unlike the majority of other artistic disciplines, music is usually considered as not being able to generate creative work directly from the concrete reality \cite{leichtentritt1945aesthetic}. As a result, style is slightly different in music than in literature, painting or sculpture. According to Dannenberg, it is almost impossible to find any obvious objective meaning, or referent associated with a short melody without words \cite{dannenberg2010style}. Essentially, every aspect of melody that communicates with listener is an aspect of style. One would say style is everything in music, or everything in music is style.

In music, like in other forms of art, style represents a classification of the medium. Likewise, a style is often associated with an era. Art historians often map the trajectory of the two in order to decipher the influence of one school of artists on another. In regards to classification with metadata, ``Computation Analysis Of Musical Influence" by Nick Collins mentions ``...sourcing data from allmusic.com, and utilising python APIs for last.fm, EchoNest, and MusicBrainz." In his approach, Collins also cross referenced the year of the primary release with Discogs.com for accuracy. This paper touches upon the idea of a more semantic understanding of the output of classifiers. 

In that effort, several papers have explored the efficacy of learning algorithms to predict genres. In his paper, George Tzanetakis effectively classified genres on live radio broadcasts using a Gaussian classifier \cite{tzanetakis2002musical}. Mandel used Support Vector Machine (hereafter referred to as \textit{SVM}) on artist and album-level features to make similar classification as well \cite{mandel2005song}. Also, another study explored mixtures of Gaussians and K-Nearest-Neighbors (hereafter referred to as \textit{KNN}) for music classification \cite{li2003comparative}. Each of these studies used similar features - Mel-Frequency Cepstral Coefficients (hereafter referred to as \textit{MFCC}) and chroma features of audio to make the classification.


%==========================================
\section{Algorithm Overview \& description}
\label{sec:algorithm}

\subsection{Audio Feature Extraction}
\label{subsec:feature}
Feature extraction is the process of computing a compact numerical representation that can be used to characterize a segment of audio \cite{tzanetakis2002musical}. In terms of MIR, the design of descriptive features for specific application is the main challenge in building pattern recognition systems.

\subsubsection{Timbral Features: Mel-Frequency Cepstral Coefficients}
\label{subsubsec:timbral}

MFCCs are perceptually motivated features that are based on the STFT.

\begin{equation*}
{\mathrm{MFCC}}(n)	= \sum\limits_{k' = 1}^{\mathcal{K}'}{\log\big( |X'(k',n)|\big)\cdot \cos\left( j\cdot\left(k'-\frac{1}{2} \right)\frac{\pi}{\mathcal{K}'} \right)}			
\end{equation*}

\subsubsection{Spectral Features}
\label{subsubsec:spectral}
The Spectral Centroid (hereafter referred to as \textit{SC}) is defined as the center of gravity of the magnitude spectrum of the STFT. The centroid is a measure of spectral shape and higher centroid values correspond to "brighter" textures with more high frequencies.

\begin{equation*}\label{eq:centroid}
\mathrm{SC}(n) = \frac{\sum\limits_{k = 0}^{\mathcal{K}/2-1}{k\cdot |X(k,n)|^2}}{\sum\limits_{k = 0}^{N/2-1}{|X(k,n)|^2}}
			\end{equation*}

The Spectral Flux (hereafter referred to as \textit{SF}) measures the amount of change of the spectral shape. It is defined as the average difference between consecutive STFT frames: (should be an equation here).


\begin{eqnarray*}
{\mathrm{SF}}(n, \beta) &=& \frac{\sqrt[\beta]{\sum\limits_{k = 0}^{\mathcal{K}/2-1}{\left(|X(k,n)|-|X(k,n-1)|\right)^\beta}}}{{\mathcal{K}}{/2}} 
\end{eqnarray*}

\subsection{Dataset Creation}
\label{subsec:dataset}
In this research, we selected GTZAN as our starting dataset. George Tzanetakis created GTZAN specifically for machine learning analysis of genre classification problems. This dataset has been documented to have non-negligible issues such as "album effect" with repeated audio samples which would negatively influence the accuracy of the classification \cite{sturm2013gtzan}, it still can be seen as the default choice in the research area of music genre classification. Figure 1 represents the degree of activity of GTZAN.

\begin{figure}[htb]
\begin{minipage}[b]{1.0\linewidth}
  \centering
  \centerline{\includegraphics[width=8.5cm]{figures/GTZAN1.png}}
  \vspace{0.0cm}
  \caption{ Annual numbers of published works in MIR with experimental components, divided into ones that use and do not use GTZAN \cite{sturm2013gtzan}}
\label{fig:figure1}
\end{minipage}
\end{figure}

We used a modified version of the GTZAN dataset augmented by metadata and preview audio clips sourced form 7digital.com. Our dataset started as 1000 songs from 10 genres but many of the audio clips lacked information. Furthermore, genre groups contained duplicate songs or entire albums. In an attempt to reduce the album effect and distribute the years represented, we reduced the dataset to 7 genres with 60 songs in each. We used the 7digital API endpoint interface to search within genres and scrape for additional preview clips to remove redundant data.

After creating the list of files with the artist and song name, metadata was then scraped from Discogs.com API using python. This metadata included artist, title, album name, genre, styles, and year of master release. Although some of the previews from 7digital represent remastered versions, we cross referenced with the Discogs master release for consistency.  Ideally, the dataset would have an even distribution of songs spanning the range of all years without repeated artists. The final dataset consists of 420 audio clips in total with genres Blues, Country, Disco, Hip Hop, Metal, Pop, Rock. Any additional track samples added from 7digital were cut to 30 seconds in length using ffmpeg. All songs were 30 second long 44.1k 16bit .wav or .au files.

\begin{figure}[htb]
\begin{minipage}[b]{1.0\linewidth}
  \centering
  \centerline{\includegraphics[width=8.5cm]{figures/YearDist2.png}}
  \vspace{0.0cm}
  \caption{ Histogram displaying the distribution of years in our modified dataset}
\label{fig:figure2}
\end{minipage}
\end{figure}


\subsection{Classification}
\label{subsec:classification}
For classification, variations of two basic statistical pattern recognition (hereafter referred to as \textit{SPR}) algorithms were used: SVM and KNN. Essentially, an SPR is applied to estimate the probability density function for the feature vectors of each class. This paper compares the suitability of these two SPR classifiers to maximize the accuracy of computing a song's year.

%===========================================
\section{Evaluation}
\label{sec:evaluation}

\subsection{Methodology}
\label{subsec:methodology}

\subsubsection{N-fold Validation}
N-fold was applied to perform multiple iterations of classification for evaluation. The dataset contains 7 genres with 60 songs each (420 songs in total). An N value of 6 evenly distributes the genres represented in the training and test sets. This way, for each fold, the training set contains 350 songs and the test set contains 70 songs, with 10 songs from each genre.

\subsubsection{Segmentation of Data}
The data was segmented into training and testing pools using two techniques. The results were quite different and tie into some fundamental concepts of dataset creation. One approach was an interleaved distribution. The second was a randomized distribution. 

For the interleaved distribution, the feature set was read in order adding one song from each genre at a time. Because there were instances of multiple songs by an artist from the same album, the distributed segmentation approach separates songs from one album occurring in both the training and test set, thus removing the risk of the ``artist effect" or ``album effect". In his paper, Seyerlehner argues that it is relatively easy to identify songs by one and the same artist using audio similarity algorithm \cite{seyerlehner2010fusing}. This effect is known as artist effect. In some cases, even album-specific production effects are reflected in the spectral representation of songs, which is respectively called "album effect". Obviously, songs by the same artist will tend to belong to the same genre, and the ability to recognize the genre by specific production effect is not what this research intend to measure. However, this also the worst case scenario, because the training and test set will always be very different from each other. 

For the randomized distribution, a seed is generated for the entire feature set and applied to all songs and metadata, completely randomizing the order. This creates a potentially even distribution of genres within each fold for the training and test sets. It greatly increases the chance of the album effect, however, because songs from the same album that were previously only in the training set now can appear in the test set as well.

\subsubsection{Feature Selection} 

After attempting to select features with forward selection, we  manually selected features, mimicking a backward selection approach. Testing logical combinations of feature groups proved to be more successful. In general, MFCCs and subsets of MFCCs outperformed other feature combinations. Our main groups were the mean and standard deviations of MFCCs, spectral centroid, and spectral flux as mentioned in section 3.1. We also tested mean and standard deviation of Pitch Chroma, however this never resulted in improvement. 

\subsection{Metrics}
We tested four variations on the KNN and SVM algorithms for each of the two segmentation approaches. For the random distribution, each attempt was the mean of the same 6-fold process described in section 4.1.1. The seed and therefore the result changed with each run, so the result given is the best of 10 runs to show the lower boundary. 

\subsubsection{SVM using Regression} 

The SVM Regression was performed on the year data only, as genres can only be classified discretely. We scaled the years from the range of 1927-2015 to a range of 0-1. Using libsvm \cite{chang2011libsvm} with regression settings of a nu-SVR with a polynomial kernel, a nu value of 0.5, and a default cost parameter of 1. Combining the mean and standard deviation of MFCCs gave the most successful results. Our average mean error in calculating the year was 12.754 years for the interleaved segmentation and 10.425 years for the randomized segmentation.  

\subsubsection{KNN of Year}

The second approach used only the KNN directly on the year label, with no genre influence. This was surprisingly the simplest but most effective result. We achieved our best results using a K value of 7 with only the mean of MFCCs as features. This resulted in a mean error of 8.751 years and 7.976 years for the interleaved segmentation and randomized segmentation respectively. Our KNN algorithm uses an inverse weighting function calculated by the euclidean distance as a method to give priority over nearer results, shown below.
\begin{eqnarray*}
w_{k} &=&  (1 - (d_{k} / {\sum\limits_{k = 1}^{K} d_{k}})) / (K-1)
 \end{eqnarray*}
\begin{eqnarray*}
{\mathrm{Y_{E}}} &=& {\sum\limits_{k = 1}^{K} w_{k}*y_{k}}
 \end{eqnarray*}
 
\subsubsection{Hierarchical Model: SVM and KNN } 
Our third approach make use of both the SVM and KNN models in a hierarchical manner. First we used the VLFeat SVM \cite{vedaldi2010vlfeat} to classify the genres using both the mean and standard deviation of MFCCs and our spectral features. Prior to refactoring our dataset, we achieved results of 55.4\% and 66.4\% accuracy for the interleaved and randomized segmentations respectively. After refactoring, however we saw our results decrease, due to the album effect. Since we intentionally targeted repeat songs and full albums present in the GTZAN dataset, our distribution of features expanded within each genre making it more difficult for the SVM to accurately perform classification. After refactoring, the accuracy for classifying genre was 51.5\% and 61.0\% for the interleaved and randomized segmentations respectively. The confusion matrices in Figure 3 describe these results.

\begin{figure*}
  \includegraphics[width=0.5\textwidth]{figures/SVM_D_Confusion.png}
  \includegraphics[width=0.5\textwidth]{figures/SVM_R_Confusion.png}
    \includegraphics[width=0.5\textwidth]{figures/SVM_D_NEW_Confusion}
  \includegraphics[width=0.5\textwidth]{figures/SVM_R_NEW_Confusion}
  \caption[width=\textwidth]
 {Confusion matrices resulting from the SVM.  \textit{Left to right}: Interleaved vs Randomized segmentation. \textit{Top to bottom}: Original GTZAN dataset vs modified dataset}
\end{figure*}


After classifying genre, we used the predicted genre for each test song and ran KNN to find the nearest neighbors to compute the year. The selection used to run KNN, however, only found the nearest neighbors within the genre predicted for each song. For example, if we predicted a song to be country, KNN only selects the country songs in the current training fold. Using a value of K=9 gave the best results. Our average mean error in calculating the year was 12.132 and 11.602 years for interleaved segmentation and randomized segmentation respectively. 

\subsubsection{Hierarchical Model: KNN with Ground Truth Genre} 

The final approach removed genre prediction and simply used the ground truth metadata of the test set. The KNN was run again for each audio clip in the test set against using the actual genre metadata for that track. Again, it limited the selection of features from the training set to those that matched the clip's genre. For consistency, we ran this test using both the mean and standard deviation of MFCCs and our spectral features, achieving an average mean error of 12.412 and 11.926 years for interleaved segmentation and randomized segmentation respectively. Interestingly enough, the result was slightly worse than using the trained genre rather than the ground truth. Realizing that optimizing the features selected for the genre when it is not actually used, we experimented with modifying the active features to tune it to computing the year. We found that using only mean MFCCs 6 through 12 resulted in a mean error of 11.136 and 11.670 years for interleaved segmentation and randomized segmentation respectively. 

%============================================
\section{conclusion}

Overall, the KNN of Year method using the mean of MFCCs as features worked best. The hierarchical model did show improvement in relation to the SVM but only slightly. It still underperformed in comparison with KNN. Our results were very dependent upon the dataset construction and any flaws within the dataset. 

Forward selection did not prove successful when using MFCCs and Spectral data. The algorithm results would decrease and return before grouping all MFCCs. Manually tuning by finding subsets and groupings of mean and standard deviation of MFCCs with the spectral features proved most effective. Adding Pitch Chroma was ineffective and only generated worse results for both genre and year. When constructing a feature set by calculating the mean and standard deviation of pitch chroma, the temporal resolution is lost.

In each genre, many of the songs are from only a few albums. Training and testing for the genre or style classification with GTZAN is more likely doing the album classification. This ``album" effect heavily influences the genre classification albeit positively, however this is actually incorrect. Essentially, by importing new songs and abandoning several songs that belong to the same album, the augmented dataset is more even distributed by year.

%============================================
\section{Future Work}
\label{sec: future}

\subsection{Dataset Modification}
In our research, the ``album effect" and the size of the dataset are two non-negligible issues.  Since the GTZAN dataset was incomplete with the basic artist and song title tags and contained duplicate songs, our resulting dataset was small. Ideally, the size of the dataset should be at least twice that of what was used. A small dataset can lead to automatic selection and neglect potentially useful features. To train a qualified year computation system, a large dataset with evenly distributed audio samples spanning many years and genres is necessary. This does take quite a long time, however the scripts provided to scrape metadata from Discogs in conjunction with the 7digital audio preview scraping may be a good solution to this problem.

\subsection{Feature Extraction}
In this research, timbral texture features and spectral features are extracted from the audio sample and used for training the system. However, rhythmic features like beat histogram are not employed. With the rhythmic features, the performance of system should improve.

\subsection{Classification}
SVM and KNN as statistical pattern recognition models are widely used in the research of MIR. Recently, deep learning methods, especially convolutional neural networks (also known as CNNs) have become more and more popular in this research area. Convolutional neural networks, characterized by a multi-layer structure, sparse connectivity, and weight sharing have already improved the performance of music genre classifiers and chord detectors. Combining CNN with audio features may help improve the accuracy of song's decade computation.

%============================================
%\section{Formatting your paper}
%\label{sec:format}
%
%All printed material, including text, illustrations, and charts, must be kept
%within a print area of 7 inches (178 mm) wide by 9 inches (229 mm) high. Do
%not write or print anything outside the print area. The top margin must be 1
%inch (25 mm), except for the title page, and the left margin must be 0.75 inch
%(19 mm).  All {\it text} must be in a two-column format. Columns are to be 3.39
%inches (86 mm) wide, with a 0.24 inch (6 mm) space between them. Text must be
%fully justified.
%
%\section{PAGE TITLE SECTION}
%\label{sec:pagestyle}
%
%The paper title (on the first page) should begin 1.38 inches (35 mm) from the
%top edge of the page, centered, completely capitalized, and in Times 14-point,
%boldface type.  The authors' name(s) and affiliation(s) appear below the title
%in capital and lower case letters.  Papers with multiple authors and
%affiliations may require two or more lines for this information. Please note
%that papers should not be submitted blind; include the authors' names on the
%PDF.
%
%\section{TYPE-STYLE AND FONTS}
%\label{sec:typestyle}
%
%To achieve the best rendering both in printed proceedings and electronic proceedings, we
%strongly encourage you to use Times-Roman font.  In addition, this will give
%the proceedings a more uniform look.  Use a font that is no smaller than nine
%point type throughout the paper, including figure captions.
%
%In nine point type font, capital letters are 2 mm high.  {\bf If you use the
%smallest point size, there should be no more than 3.2 lines/cm (8 lines/inch)
%vertically.}  This is a minimum spacing; 2.75 lines/cm (7 lines/inch) will make
%the paper much more readable.  Larger type sizes require correspondingly larger
%vertical spacing.  Please do not double-space your paper.  TrueType or
%Postscript Type 1 fonts are preferred.
%
%The first paragraph in each section should not be indented, but all the
%following paragraphs within the section should be indented as these paragraphs
%demonstrate.
%
%\section{MAJOR HEADINGS}
%\label{sec:majhead}
%
%Major headings, for example, "1. Introduction", should appear in all capital
%letters, bold face if possible, centered in the column, with one blank line
%before, and one blank line after. Use a period (".") after the heading number,
%not a colon.
%
%\subsection{Subheadings}
%\label{ssec:subhead}
%
%Subheadings should appear in lower case (initial word capitalized) in
%boldface.  They should start at the left margin on a separate line.
% 
%\subsubsection{Sub-subheadings}
%\label{sssec:subsubhead}
%
%Sub-subheadings, as in this paragraph, are discouraged. However, if you
%must use them, they should appear in lower case (initial word
%capitalized) and start at the left margin on a separate line, with paragraph
%text beginning on the following line.  They should be in italics.
%
%\section{PRINTING YOUR PAPER}
%\label{sec:print}
%
%Print your properly formatted text on high-quality, 8.5 x 11-inch white printer
%paper. A4 paper is also acceptable, but please leave the extra 0.5 inch (12 mm)
%empty at the BOTTOM of the page and follow the top and left margins as
%specified.  If the last page of your paper is only partially filled, arrange
%the columns so that they are evenly balanced if possible, rather than having
%one long column.
%
%In LaTeX, to start a new column (but not a new page) and help balance the
%last-page column lengths, you can use the command ``$\backslash$pagebreak'' as
%demonstrated on this page (see the LaTeX source below).
%
%\section{PAGE NUMBERING}
%\label{sec:page}
%
%Please do {\bf not} paginate your paper.  Page numbers, session numbers, and
%conference identification will be inserted when the paper is included in the
%proceedings.
%
%\section{ILLUSTRATIONS, GRAPHS, AND PHOTOGRAPHS}
%\label{sec:illust}
%
%Illustrations must appear within the designated margins.  They may span the two
%columns.  If possible, position illustrations at the top of columns, rather
%than in the middle or at the bottom.  Caption and number every illustration.
%All halftone illustrations must be clear black and white prints.  Colors may be
%used, but they should be selected so as to be readable when printed on a
%black-only printer.
%
%Since there are many ways, often incompatible, of including images (e.g., with
%experimental results) in a LaTeX document, below is an example of how to do
%this \cite{Lamp86}.
%
%\section{FOOTNOTES}
%\label{sec:foot}
%
%Use footnotes sparingly (or not at all!) and place them at the bottom of the
%column on the page on which they are referenced. Use Times 9-point type,
%single-spaced. To help your readers, avoid using footnotes altogether and
%include necessary peripheral observations in the text (within parentheses, if
%you prefer, as in this sentence).

% Below is an example of how to insert images. Delete the ``\vspace'' line,
% uncomment the preceding line ``\centerline...'' and replace ``imageX.ps''
% with a suitable PostScript file name.
% -------------------------------------------------------------------------
%\begin{figure}[htb]
%
%\begin{minipage}[b]{1.0\linewidth}
%  \centering
%  \centerline{\includegraphics[width=8.5cm]{image1}}
%%  \vspace{2.0cm}
%  \centerline{(a) Result 1}\medskip
%\end{minipage}
%%
%\begin{minipage}[b]{.48\linewidth}
%  \centering
%  \centerline{\includegraphics[width=4.0cm]{image3}}
%%  \vspace{1.5cm}
%  \centerline{(b) Results 3}\medskip
%\end{minipage}
%\hfill
%\begin{minipage}[b]{0.48\linewidth}
%  \centering
%  \centerline{\includegraphics[width=4.0cm]{image4}}
%%  \vspace{1.5cm}
%  \centerline{(c) Result 4}\medskip
%\end{minipage}
%%
%\caption{Example of placing a figure with experimental results.}
%\label{fig:res}
%%
%\end{figure}


% To start a new column (but not a new page) and help balance the last-page
% column length use \vfill\pagebreak.
% -------------------------------------------------------------------------
%\vfill
%\pagebreak

%\section{COPYRIGHT FORMS}
%\label{sec:copyright}
%
%You must submit your fully completed, signed IEEE electronic copyright release
%form when you submit your paper. We {\bf must} have this form before your paper
%can be published in the proceedings.
%
%\section{RELATION TO PRIOR WORK}
%\label{sec:prior}
%
%The text of the paper should contain discussions on how the paper's
%contributions are related to prior work in the field. It is important
%to put new work in  context, to give credit to foundational work, and
%to provide details associated with the previous work that have appeared
%in the literature. This discussion may be a separate, numbered section
%or it may appear elsewhere in the body of the manuscript, but it must
%be present.
%
%You should differentiate what is new and how your work expands on
%or takes a different path from the prior studies. An example might
%read something to the effect: "The work presented here has focused
%on the formulation of the ABC algorithm, which takes advantage of
%non-uniform time-frequency domain analysis of data. The work by
%Smith and Cohen \cite{Lamp86} considers only fixed time-domain analysis and
%the work by Jones et al \cite{C2} takes a different approach based on
%fixed frequency partitioning. While the present study is related
%to recent approaches in time-frequency analysis [3-5], it capitalizes
%on a new feature space, which was not considered in these earlier
%studies."

\vfill\pagebreak

%\section{REFERENCES}
%\label{sec:refs}
%
%List and number all bibliographical references at the end of the
%paper. The references can be numbered in alphabetic order or in
%order of appearance in the document. When referring to them in
%the text, type the corresponding reference number in square
%brackets as shown at the end of this sentence \cite{C2}. An
%additional final page (the fifth page, in most cases) is
%allowed, but must contain only references to the prior
%literature.

% References should be produced using the bibtex program from suitable
% BiBTeX files (here: strings, refs, manuals). The IEEEbib.bst bibliography
% style fiåle from IEEE produces unsorted bibliography list.
% -------------------------------------------------------------------------
\newpage
\newpage
\bibliographystyle{IEEEbib}
%\bibliographystyle{ieeetr}
\bibliography{refs}

\end{document}
